# -*- coding: utf-8 -*-
"""Document_Chatbot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TK4ujDIPEWLS_ZB2cJdV0l4xnh0QaXUT
"""



"""# Libraries installation and Knowledge base download"""

!apt-get install portaudio19-dev python-all-dev

!pip install openai langchain chromadb tiktoken lark panel docarray evaluate pyttsx3 SpeechRecognition PyAudio ffmpeg-python

!wget https://raw.githubusercontent.com/niravnb/knowledge_base_chatbot/main/KnowledgeDocument\(pan_card_services\).txt

"""## Open AI API Key setup"""

# Commented out IPython magic to ensure Python compatibility.
import os
import openai
import sys


# %env OPENAI_API_KEY=YOUR_OPEN_AI_KEY
openai.api_key  = 'YOUR_OPEN_AI_KEY'

"""# Document Loading"""

from langchain.document_loaders import TextLoader
loader = TextLoader('KnowledgeDocument(pan_card_services).txt')
documents = loader.load()

len(documents)

print(documents[0].page_content[0:500])

print(documents[0].metadata)



"""# Document Splitting

Context aware splitting
Chunking aims to keep text with common context together.

A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.

I used RecursiveCharacterTextSplitter to preserve header metadata in our chunks, as show below.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter

# Char-level splits
chunk_size = 1000
chunk_overlap = 50
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap,
                                               separators=["#", "##", "###","\n\n", "\n", "(?<=\. )", " ", ""])

# Split
splits = text_splitter.split_documents(documents)

len(splits)

splits[0]

splits[11]



"""# Vectorstores and Embeddings

Let's take our splits and embed them.


"""

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings()

from langchain.vectorstores import Chroma
persist_directory = 'docs/chroma/'

!rm -rf ./docs/chroma  # remove old database files if any

vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)

print(vectordb._collection.count())

vectordb.persist()

"""# Similarity Search

For sanity checks
"""

question = "what is PAN card?"

docs = vectordb.similarity_search(question,k=2) # k is number of documents
docs





"""# Loading db from memory"""

from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)

print(vectordb._collection.count())



"""# Creating Memory for chatbot"""

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

"""# Retrieval
Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow.
"""

llm_name = 'gpt-3.5-turbo'
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)
llm.predict("Hello world!")

# Build prompt
from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

from langchain.chains import ConversationalRetrievalChain

qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
    memory=memory,
    chain_type="stuff",
)

question = "describe PAN card"
result = qa({"question": question})
result['answer']

question = "what is the purpose of PAN card"
result = qa({"question": question})
result['answer']

question = "why do i need it?"
result = qa({"question": question})
result['answer']

question = "How to apply for it"
result = qa({"question": question})
result['answer']

question = "What are the documents required to link PAN with Aadhaar"
result = qa({"question": question})
print(result['answer'])





"""# UI"""

import panel as pn  # GUI
pn.extension()

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.document_loaders import TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader

# This will initialize your database and retriever chain
def load_db(file, chain_type, k):
    # load documents
    loader =  TextLoader(file)
    documents = loader.load()
    # split documents
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separators=["#", "##", "###","\n\n", "\n", "(?<=\. )", " ", ""])
    docs = text_splitter.split_documents(documents)
    # define embedding
    embeddings = OpenAIEmbeddings()
    # create vector database from data
    db = DocArrayInMemorySearch.from_documents(docs, embeddings)
    # define retriever
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    # create a chatbot chain. Memory is managed externally.
    qa = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name=llm_name, temperature=0),
        chain_type=chain_type,
        retriever=retriever,
        return_source_documents=True,
        return_generated_question=True,
    )
    return qa

import panel as pn
import param

class cbfs(param.Parameterized):
    chat_history = param.List([])
    answer = param.String("")
    db_query  = param.String("")
    db_response = param.List([])

    def __init__(self,  **params):
        super(cbfs, self).__init__( **params)
        self.panels = []
        self.loaded_file = "KnowledgeDocument(pan_card_services).txt"
        self.qa = load_db(self.loaded_file,"stuff", 4)

    def call_load_db(self, count):
        if count == 0 or file_input.value is None:  # init or no file specified :
            return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")
        else:
            file_input.save("temp.pdf")  # local copy
            self.loaded_file = file_input.filename
            button_load.button_style="outline"
            self.qa = load_db("temp.pdf", "stuff", 4)
            button_load.button_style="solid"
        self.clr_history()
        return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")

    def convchain(self, query):
        if not query:
            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown("", width=600)), scroll=True)
        result = self.qa({"question": query, "chat_history": self.chat_history})
        self.chat_history.extend([(query, result["answer"])])
        self.db_query = result["generated_question"]
        self.db_response = result["source_documents"]
        self.answer = result['answer']
        self.panels.extend([
            pn.Row('User:', pn.pane.Markdown(query, width=600)),
            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))
        ])
        inp.value = ''  #clears loading indicator when cleared
        return pn.WidgetBox(*self.panels,scroll=True)

    @param.depends('db_query ', )
    def get_lquest(self):
        if not self.db_query :
            return pn.Column(
                pn.Row(pn.pane.Markdown(f"Last question to DB:", styles={'background-color': '#F6F6F6'})),
                pn.Row(pn.pane.Str("no DB accesses so far"))
            )
        return pn.Column(
            pn.Row(pn.pane.Markdown(f"DB query:", styles={'background-color': '#F6F6F6'})),
            pn.pane.Str(self.db_query )
        )

    @param.depends('db_response', )
    def get_sources(self):
        if not self.db_response:
            return
        rlist=[pn.Row(pn.pane.Markdown(f"Result of DB lookup:", styles={'background-color': '#F6F6F6'}))]
        for doc in self.db_response:
            rlist.append(pn.Row(pn.pane.Str(doc)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    @param.depends('convchain', 'clr_history')
    def get_chats(self):
        if not self.chat_history:
            return pn.WidgetBox(pn.Row(pn.pane.Str("No History Yet")), width=600, scroll=True)
        rlist=[pn.Row(pn.pane.Markdown(f"Current Chat History variable", styles={'background-color': '#F6F6F6'}))]
        for exchange in self.chat_history:
            rlist.append(pn.Row(pn.pane.Str(exchange)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    def clr_history(self,count=0):
        self.chat_history = []
        return

import warnings
warnings.filterwarnings('ignore')

cb = cbfs()

file_input = pn.widgets.FileInput(accept='.txt')
button_load = pn.widgets.Button(name="Load DB", button_type='primary')
button_clearhistory = pn.widgets.Button(name="Clear History", button_type='warning')
button_clearhistory.on_click(cb.clr_history)
inp = pn.widgets.TextInput( placeholder='Enter text here…')

bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)
conversation = pn.bind(cb.convchain, inp)

tab1 = pn.Column(
    pn.Row(inp),
    pn.layout.Divider(),
    pn.panel(conversation,  loading_indicator=True, height=300),
    pn.layout.Divider(),
)
tab2= pn.Column(
    pn.panel(cb.get_lquest),
    pn.layout.Divider(),
    pn.panel(cb.get_sources ),
)
tab3= pn.Column(
    pn.panel(cb.get_chats),
    pn.layout.Divider(),
)
tab4=pn.Column(

    pn.Row( file_input, button_load, bound_button_load),
    pn.Row( button_clearhistory, pn.pane.Markdown("Clears chat history. Can use to start a new topic" )),
    pn.layout.Divider(),
)
dashboard = pn.Column(
    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),
    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))
)
dashboard





"""# Evaluation of the answers"""

!wget https://github.com/niravnb/knowledge_base_chatbot/blob/main/SampleQuestions.csv

import pandas as pd

df = pd.read_csv('SampleQuestions.csv')[['Question','Ideal Answer']]
df = df.rename(columns={'Question':"question", "Ideal Answer": "answer"})
df.head()

df = df.sample(10)
df

dct = df.T.to_dict()
examples = []
for _, v in dct.items():
  examples.append(v)

examples



"""## Predictions"""

from langchain.chains import RetrievalQA

# Run chain
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

import time

predictions = []
for i in range(len(examples)):
  result = qa_chain({"query": examples[i]['question']})
  predictions.append({'text' : result['result']})
  time.sleep(10)

predictions



"""## Evaluation

We can see that if we tried to just do exact match on the answer answers they would not match what the language model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers.


"""

from langchain.evaluation.qa import QAEvalChain

llm = OpenAI(temperature=0)
eval_chain = QAEvalChain.from_llm(llm)

graded_outputs = eval_chain.evaluate(
    examples, predictions, question_key="question", prediction_key="text"
)

for i, eg in enumerate(examples):
    print(f"Example {i}:")
    print("Question: " + eg["question"])
    print("Real Answer: " + eg["answer"])
    print("Predicted Answer: " + predictions[i]["text"])
    print("Predicted Grade: " + graded_outputs[i]["text"])
    print()



"""## Customizing Prompt for getting score

We can also customize the prompt that is used. Here is an example prompting it using a score from 0 to 10. The custom prompt requires 3 input variables: "query", "answer" and "result". Where "query" is the question, "answer" is the ground truth answer, and "result" is the predicted answer.


"""

from langchain.prompts.prompt import PromptTemplate

_PROMPT_TEMPLATE = """Imagine you are an expert at grading and answering to questions related to PAN card.
You are grading the following question:
{query}
Here is the real answer:
{answer}
You are grading the following predicted answer:
{result}
What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?
"""

PROMPT = PromptTemplate(
    input_variables=["query", "answer", "result"], template=_PROMPT_TEMPLATE
)

evalchain = QAEvalChain.from_llm(llm=llm, prompt=PROMPT)
evalchain.evaluate(
    examples,
    predictions,
    question_key="question",
    answer_key="answer",
    prediction_key="text",
)



from langchain.prompts.prompt import PromptTemplate

_PROMPT_TEMPLATE = """You are an expert at grading and answering to questions related to PAN card.
You are grading the following question:
{query}
Here is the real answer:
{answer}
You are grading the following predicted answer:
{result}
What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?
Output just a grade number, nothing else.
"""

PROMPT = PromptTemplate(
    input_variables=["query", "answer", "result"], template=_PROMPT_TEMPLATE
)

evalchain = QAEvalChain.from_llm(llm=llm, prompt=PROMPT)
evalchain.evaluate(
    examples,
    predictions,
    question_key="question",
    answer_key="answer",
    prediction_key="text",
)

"""## Comparing to SQUAD evaluation metric"""

# Some data munging to get the examples in the right format
for i, eg in enumerate(examples):
    eg["id"] = str(i)
    eg["answers"] = {"text": [eg["answer"]], "answer_start": [0]}
    predictions[i]["id"] = str(i)
    predictions[i]["prediction_text"] = predictions[i]["text"]

for p in predictions:
    del p["text"]

new_examples = examples.copy()
for eg in new_examples:
    del eg["question"]
    del eg["answer"]

from evaluate import load

squad_metric = load("squad")
results = squad_metric.compute(
    references=new_examples,
    predictions=predictions,
)

results





"""# Multi-lingual support"""

def multi_lingual_chatbot(question):
  multi_prompt = f"""
  If the question asked below delimieted by triple backticks, is not in english language,\
  first translate the question to english, then get the answer for the question, \
  and then translate the response back to the language in which the question is asked.

  Question: ```{question}```

  Output only the response in the language of the question asked and nothing else.
  """
  result = qa({"question": multi_prompt})
  return result['answer']

question = "What are the documents required to link PAN with Aadhaar"
multi_lingual_chatbot(question)

question = "पैन को आधार से लिंक करने के लिए कौन से दस्तावेज़ आवश्यक हैं?"
multi_lingual_chatbot(question)

question = "ਇੱਕ ਐਨਆਰਆਈ ਨਵੇਂ ਪੈਨ ਕਾਰਡ ਲਈ ਅਰਜ਼ੀ ਕਿਵੇਂ ਦੇ ਸਕਦਾ ਹੈ?"
multi_lingual_chatbot(question)

question = "ਕੀ ਮੈਂ ਭਾਰਤੀ ਪਤੇ 'ਤੇ ਪੈਨ ਕਾਰਡ ਦੀ ਡਿਲਿਵਰੀ ਲੈ ਸਕਦਾ ਹਾਂ?"
multi_lingual_chatbot(question)

question = "હું પાન કાર્ડ પર મારા પિતાનું નામ બદલવા માંગુ છું શું પ્રક્રિયા છે?"
multi_lingual_chatbot(question)





"""# Voice support: Adding speech capabilities  - taking questions in voice, and outputting responses in voice"""

import speech_recognition as sr
import pyttsx3

engine = pyttsx3.init()


def listen():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        print("Calibrating...")
        r.adjust_for_ambient_noise(source, duration=5)
        # optional parameters to adjust microphone sensitivity
        # r.energy_threshold = 200
        # r.pause_threshold=0.5

        print("Okay, go!")
        while 1:
            text = ""
            print("listening now...")
            try:
                audio = r.listen(source, timeout=5, phrase_time_limit=30)
                print("Recognizing...")
                # whisper model options are found here: https://github.com/openai/whisper#available-models-and-languages
                # other speech recognition models are also available.
                text = r.recognize_whisper(
                    audio,
                    model="medium.en",
                    show_dict=True,
                )["text"]
            except Exception as e:
                unrecognized_speech_text = (
                    f"Sorry, I didn't catch that. Exception was: {e}s"
                )
                text = unrecognized_speech_text
            print(text)

            result = qa({"question": question})
            response_text = result['answer']
            print(response_text)
            engine.say(response_text)
            engine.runAndWait()

listen()



